WDriver.java
package countip;


import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapred.*;

import java.io.IOException;

public class WDriver {
public static void main(String[] args) {
JobClient my_client = new JobClient();
JobConf job_conf = new JobConf(WDriver.class);
job_conf.setJobName("SalePerCountry");
job_conf.setOutputKeyClass(Text.class);
job_conf.setOutputValueClass(IntWritable.class);
job_conf.setMapperClass(countip.WMapper.class);
job_conf.setReducerClass(countip.WReducer.class);
job_conf.setInputFormat(TextInputFormat.class);
job_conf.setOutputFormat(TextOutputFormat.class);

// Set input and output directories using command line arguments,
//arg[0] = name of input directory on HDFS, and arg[1] =  name of output directory to be created to store the output file.

FileInputFormat.setInputPaths(job_conf, new Path(args[0]));
FileOutputFormat.setOutputPath(job_conf, new Path(args[1]));

my_client.setConf(job_conf);
try {
// Run the job
JobClient.runJob(job_conf);
} catch (Exception e) {
e.printStackTrace();
}
Configuration conf = new Configuration();
try
{
FileSystem fs = FileSystem.get(conf);
Path p = new Path("hdfs://localhost:9000"+args[1]+"part-00000");
FSDataInputStream in = fs.open(new Path("hdfs://localhost:9000"+args[1]+"/part-00000"));
int max=0,i=1;
String ip,l;
String fip = "";

if(!fs.exists(p))
{
System.out.println("File exits");

while((l = in.readLine()) != null)
{
String[] arr = l.split("\t");
if(Integer.parseInt(arr[1]) > max)
{
max = Integer.parseInt(arr[1]);
fip = arr[0];
}
}

}
System.out.println("===========Maximum Ocuurace of IP===========");
System.out.println("IP : " +fip);
System.out.println("Count : " +max);




}catch(IOException e){
e.printStackTrace();
}



}
}



WMapper.java

package countip;

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.*;
//import java.util.StringTokenizer;

public class WMapper extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable>
{

public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException
{
   
    Text word = new Text();
       String line = value.toString();  
       
      String[] lineSplit = line.split(",");
      String ip = lineSplit[0];        
       
      if(ip.startsWith("10"))
       
      output.collect(new Text(ip), new IntWritable(1));            
               
   }    
   
}
